{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    " \n",
    "import P3_helpers as hlp\n",
    "from P3_helpers import get_dataframe, get_parsed_comment, get_LDA_model, tokenize_one_comment, get_LDA_model_from_saved_file\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and preprocessing DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Source', 'Target', 'Vote', 'Results', 'Year', 'Date', 'Comment'], dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = get_dataframe()\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Parsed_Comment'] = df['Comment'].apply(get_parsed_comment)\n",
    "df['Tokenized_Comment_not_filtered'] = df['Parsed_Comment'].apply(tokenize_one_comment)\n",
    "df['BoW'] = hlp.get_bow_column(df['Tokenized_Comment_not_filtered'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing and saving topic models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb_topics: 3\n",
      "Saving model for 3 topics\n",
      "Topic_3 done\n",
      "nb_topics: 5\n",
      "Saving model for 5 topics\n",
      "Topic_5 done\n",
      "nb_topics: 7\n",
      "Saving model for 7 topics\n",
      "Topic_7 done\n",
      "nb_topics: 9\n",
      "Saving model for 9 topics\n",
      "Topic_9 done\n"
     ]
    }
   ],
   "source": [
    "isTrainSession = False\n",
    "\n",
    "if isTrainSession:\n",
    "    comments_series = df['Parsed_Comment'].copy(deep=True)\n",
    "    topic_range = range(3, 10, 2)\n",
    "    for nb_topics in topic_range:\n",
    "        print(f\"nb_topics: {nb_topics}\")\n",
    "        lda = get_LDA_model(comments_series=comments_series, num_topics=nb_topics)\n",
    "        print(f\"Saving model for {nb_topics} topics\")\n",
    "        lda.save(fname=f\"lda_model_{nb_topics}_all_true_lda\")\n",
    "        print(f\"Topic_{nb_topics} done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading pre-computed models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "nb_topics = range(3, 10, 2)\n",
    "\n",
    "for nb_topic in nb_topics:\n",
    "    models.append(get_LDA_model_from_saved_file(f\"./topic_model_states_dict/lda_model_{nb_topic}_all_true_lda\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Infer Topics by Comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, model in enumerate(models):\n",
    "    df[f'Topics_from_{nb_topics[i]}'] = df['BoW'].apply(model.get_document_topics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('df_with_topics.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'int' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m i, models \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(models):\n\u001b[0;32m----> 2\u001b[0m     d \u001b[39m=\u001b[39m model[i]\u001b[39m.\u001b[39mprint_topics(num_words\u001b[39m=\u001b[39m\u001b[39m15\u001b[39m)\n\u001b[1;32m      3\u001b[0m     d \u001b[39m=\u001b[39m {topic[\u001b[39m0\u001b[39m]:topic[\u001b[39m1\u001b[39m] \u001b[39mfor\u001b[39;00m topic \u001b[39min\u001b[39;00m d}\n\u001b[1;32m      4\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtopics_\u001b[39m\u001b[39m{\u001b[39;00mnb_topics[i]\u001b[39m}\u001b[39;00m\u001b[39m_dict.json\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n",
      "File \u001b[0;32m~/anaconda3/envs/ada/lib/python3.9/site-packages/gensim/models/ldamodel.py:1549\u001b[0m, in \u001b[0;36mLdaModel.__getitem__\u001b[0;34m(self, bow, eps)\u001b[0m\n\u001b[1;32m   1528\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, bow, eps\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m   1529\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Get the topic distribution for the given document.\u001b[39;00m\n\u001b[1;32m   1530\u001b[0m \n\u001b[1;32m   1531\u001b[0m \u001b[39m    Wraps :meth:`~gensim.models.ldamodel.LdaModel.get_document_topics` to support an operator style call.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1547\u001b[0m \n\u001b[1;32m   1548\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1549\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_document_topics(bow, eps, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mminimum_phi_value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mper_word_topics)\n",
      "File \u001b[0;32m~/anaconda3/envs/ada/lib/python3.9/site-packages/gensim/models/ldamodel.py:1354\u001b[0m, in \u001b[0;36mLdaModel.get_document_topics\u001b[0;34m(self, bow, minimum_probability, minimum_phi_value, per_word_topics)\u001b[0m\n\u001b[1;32m   1347\u001b[0m     kwargs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\n\u001b[1;32m   1348\u001b[0m         per_word_topics\u001b[39m=\u001b[39mper_word_topics,\n\u001b[1;32m   1349\u001b[0m         minimum_probability\u001b[39m=\u001b[39mminimum_probability,\n\u001b[1;32m   1350\u001b[0m         minimum_phi_value\u001b[39m=\u001b[39mminimum_phi_value\n\u001b[1;32m   1351\u001b[0m     )\n\u001b[1;32m   1352\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_apply(corpus, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m-> 1354\u001b[0m gamma, phis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minference([bow], collect_sstats\u001b[39m=\u001b[39;49mper_word_topics)\n\u001b[1;32m   1355\u001b[0m topic_dist \u001b[39m=\u001b[39m gamma[\u001b[39m0\u001b[39m] \u001b[39m/\u001b[39m \u001b[39msum\u001b[39m(gamma[\u001b[39m0\u001b[39m])  \u001b[39m# normalize distribution\u001b[39;00m\n\u001b[1;32m   1357\u001b[0m document_topics \u001b[39m=\u001b[39m [\n\u001b[1;32m   1358\u001b[0m     (topicid, topicvalue) \u001b[39mfor\u001b[39;00m topicid, topicvalue \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(topic_dist)\n\u001b[1;32m   1359\u001b[0m     \u001b[39mif\u001b[39;00m topicvalue \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m minimum_probability\n\u001b[1;32m   1360\u001b[0m ]\n",
      "File \u001b[0;32m~/anaconda3/envs/ada/lib/python3.9/site-packages/gensim/models/ldamodel.py:697\u001b[0m, in \u001b[0;36mLdaModel.inference\u001b[0;34m(self, chunk, collect_sstats)\u001b[0m\n\u001b[1;32m    695\u001b[0m epsilon \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mfinfo(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype)\u001b[39m.\u001b[39meps\n\u001b[1;32m    696\u001b[0m \u001b[39mfor\u001b[39;00m d, doc \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(chunk):\n\u001b[0;32m--> 697\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39;49m(doc) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(doc[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m], integer_types):\n\u001b[1;32m    698\u001b[0m         \u001b[39m# make sure the term IDs are ints, otherwise np will get upset\u001b[39;00m\n\u001b[1;32m    699\u001b[0m         ids \u001b[39m=\u001b[39m [\u001b[39mint\u001b[39m(idx) \u001b[39mfor\u001b[39;00m idx, _ \u001b[39min\u001b[39;00m doc]\n\u001b[1;32m    700\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'int' has no len()"
     ]
    }
   ],
   "source": [
    "for i, model in enumerate(models):\n",
    "    d = model.print_topics(num_words=15)\n",
    "    d = {topic[0]:topic[1] for topic in d}\n",
    "    with open(f\"topics_{nb_topics[i]}_dict.json\", 'w') as f:\n",
    "        json.dump(d,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Topics_from_3 = df.Topics_from_3.apply(lambda themes_list: [(models[0].show_topic(item[0],topn=15), item[1]) for item in themes_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Topics_from_5 = df.Topics_from_5.apply(lambda themes_list: [(models[1].show_topic(item[0],topn=15), item[1]) for item in themes_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df\u001b[39m.\u001b[39mTopics_from_7 \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mTopics_from_7\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m themes_list: [(models[\u001b[39m2\u001b[39m]\u001b[39m.\u001b[39mshow_topic(item[\u001b[39m0\u001b[39m],topn\u001b[39m=\u001b[39m\u001b[39m15\u001b[39m), item[\u001b[39m1\u001b[39m]) \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m themes_list])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.Topics_from_7 = df.Topics_from_7.apply(lambda themes_list: [(models[2].show_topic(item[0],topn=15), item[1]) for item in themes_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Topics_from_9 = df.Topics_from_9.apply(lambda themes_list: [(models[3].show_topic(item[0],topn=15), item[1]) for item in themes_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([('support', 0.26539585),\n",
       "   ('good', 0.052981544),\n",
       "   ('admin', 0.0238177),\n",
       "   ('user', 0.021110583),\n",
       "   ('editor', 0.018061612),\n",
       "   ('great', 0.017362958),\n",
       "   ('strong', 0.015362159),\n",
       "   ('—', 0.013216087),\n",
       "   ('excellent', 0.010478184),\n",
       "   ('contributor', 0.009782508),\n",
       "   ('seen', 0.008929965),\n",
       "   ('work', 0.008561838),\n",
       "   ('like', 0.008408844),\n",
       "   ('tools', 0.007776866),\n",
       "   ('looks', 0.0075895917)],\n",
       "  0.7776697),\n",
       " ([('oppose', 0.03404599),\n",
       "   ('edits', 0.03332886),\n",
       "   ('wikipedia', 0.019860066),\n",
       "   ('edit', 0.018645601),\n",
       "   ('experience', 0.014715313),\n",
       "   ('good', 0.012932773),\n",
       "   ('admin', 0.0126834065),\n",
       "   ('months', 0.012108943),\n",
       "   ('user', 0.011974941),\n",
       "   ('work', 0.010490522),\n",
       "   ('articles', 0.010187494),\n",
       "   ('time', 0.010005021),\n",
       "   ('need', 0.008791911),\n",
       "   ('like', 0.008295264),\n",
       "   ('article', 0.008207716)],\n",
       "  0.11116561),\n",
       " ([('oppose', 0.025571983),\n",
       "   ('vote', 0.010133443),\n",
       "   ('admin', 0.009774776),\n",
       "   ('user', 0.009733658),\n",
       "   ('think', 0.008607526),\n",
       "   ('neutral', 0.00684812),\n",
       "   ('rfa', 0.0065301857),\n",
       "   ('time', 0.0064934324),\n",
       "   ('like', 0.006048775),\n",
       "   ('page', 0.0055847974),\n",
       "   ('adminship', 0.005446683),\n",
       "   ('people', 0.0052688224),\n",
       "   ('...', 0.004824358),\n",
       "   ('comments', 0.0047361157),\n",
       "   (\"'ve\", 0.00462037)],\n",
       "  0.111164615)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.to_csv('df_with_topics_v2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draft 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb_topics: 3, nb_words: 5\n",
      "nb_topics: 3, nb_words: 10\n",
      "nb_topics: 3, nb_words: 15\n",
      "nb_topics: 5, nb_words: 5\n",
      "nb_topics: 5, nb_words: 10\n",
      "nb_topics: 5, nb_words: 15\n",
      "nb_topics: 7, nb_words: 5\n",
      "nb_topics: 7, nb_words: 10\n",
      "nb_topics: 7, nb_words: 15\n",
      "nb_topics: 9, nb_words: 5\n",
      "nb_topics: 9, nb_words: 10\n",
      "nb_topics: 9, nb_words: 15\n"
     ]
    }
   ],
   "source": [
    "topic_range = range(3, 10, 2)\n",
    "nb_words = 15\n",
    "\n",
    "for nb_topics in topic_range:\n",
    "    print(f\"nb_topics: {nb_topics}, nb_words: {nb_words}\")\n",
    "    current_topics = get_LDA_topics_pipeline(comments_series, num_topics=nb_topics)\n",
    "    with open(f\"nbTopics_{nb_topics}_nbWords_{nb_words}.json\", \"w\") as f:\n",
    "        json.dump(current_topics, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0,\n",
       "  '0.031*\"oppose\" + 0.015*\"edits\" + 0.011*\"admin\" + 0.010*\"user\" + 0.010*\"wikipedia\" + 0.009*\"edit\" + 0.008*\"time\" + 0.008*\"neutral\" + 0.007*\"think\" + 0.007*\"like\"'],\n",
       " [1,\n",
       "  '0.223*\"support\" + 0.049*\"good\" + 0.022*\"admin\" + 0.020*\"user\" + 0.017*\"great\" + 0.016*\"editor\" + 0.014*\"strong\" + 0.014*\"—\" + 0.009*\"contributor\" + 0.009*\"work\"']]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"./topic_raw_resuls/nbTopics_2_nbWords_10.json\", \"r\") as file:\n",
    "    topics = json.load(file)\n",
    "topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draft 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['support', 'as', 'co-nom', '.'], ['support', 'as', 'nominator.', '--']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_comments = hlp.tokenize_comments(comments_series)\n",
    "tokenize_comments[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = list(STOPWORDS)\n",
    "tokenize_comments = [[word for word in comment if word not in STOPWORDS] for comment in tokenize_comments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "PONCTUATION = string.punctuation\n",
    "tokenize_comments = [[word for word in comment if word not in PONCTUATION] for comment in tokenize_comments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['support', 'co-nom'], ['support', 'nominator.', '--']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_comments[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_r = hlp.get_dict_representation(tokenize_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = hlp.get_bow_representation(tokenize_comments,d_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = hlp.init_LDA_model(bow, d_r)\n",
    "topics = hlp.get_LDA_topics(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,\n",
       " '0.221*\"support\" + 0.086*\"--\" + 0.051*\"good\" + 0.024*\"admin\" + 0.021*\"user\" + 0.017*\"editor\" + 0.017*\"great\" + 0.014*\"strong\" + 0.010*\"work\" + 0.010*\"contributor\"')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_3t = topics\n",
    "topics_3t[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = hlp.init_LDA_model(bow, d_r, num_topics=6)\n",
    "topics_6t = hlp.get_LDA_topics(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.045*\"n\\'t\" + 0.039*\"\\'s\" + 0.028*\"\\'m\" + 0.022*\"\\'ve\" + 0.022*\"time\" + 0.021*\"support\" + 0.021*\"oppose\" + 0.018*\"neutral\" + 0.017*\"think\" + 0.017*\"vote\"')\n",
      "(1, '0.243*\"support\" + 0.104*\"--\" + 0.062*\"good\" + 0.023*\"editor\" + 0.022*\"admin\" + 0.021*\"great\" + 0.020*\"user\" + 0.018*\"strong\" + 0.013*\"work\" + 0.012*\"contributor\"')\n",
      "(2, '0.024*\"\\'s\" + 0.019*\"oppose\" + 0.014*\"``\" + 0.012*\"user\" + 0.011*\"page\" + 0.011*\"wikipedia\" + 0.007*\"articles\" + 0.007*\"talk\" + 0.006*\"admin\" + 0.005*\"users\"')\n",
      "(3, '0.255*\"\\'\\'\" + 0.058*\"font\" + 0.036*\"support\" + 0.029*\"color=\" + 0.028*\"style=\" + 0.026*\"small\" + 0.019*\"vandal\" + 0.018*\"—\" + 0.018*\"span\" + 0.014*\"``\"')\n",
      "(4, '0.069*\"support\" + 0.040*\"tools\" + 0.037*\"admin\" + 0.035*\"user\" + 0.031*\"reason\" + 0.027*\"n\\'t\" + 0.024*\"use\" + 0.023*\"--\" + 0.019*\"abuse\" + 0.014*\"big\"')\n",
      "(5, '0.051*\"oppose\" + 0.046*\"edits\" + 0.026*\"edit\" + 0.020*\"--\" + 0.020*\"experience\" + 0.016*\"wikipedia\" + 0.014*\"n\\'t\" + 0.012*\"months\" + 0.011*\"user\" + 0.011*\"admin\"')\n"
     ]
    }
   ],
   "source": [
    "for topic in topics_6t:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = hlp.init_LDA_model(bow, d_r, num_topics=9)\n",
    "topics_9t = hlp.get_LDA_topics(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.284*\"support\" + 0.110*\"--\" + 0.073*\"good\" + 0.025*\"editor\" + 0.023*\"great\" + 0.021*\"user\" + 0.020*\"strong\" + 0.019*\"admin\" + 0.014*\"excellent\" + 0.013*\"work\"')\n",
      "(1, '0.059*\"edits\" + 0.050*\"oppose\" + 0.033*\"edit\" + 0.021*\"user\" + 0.021*\"months\" + 0.020*\"--\" + 0.020*\"talk\" + 0.020*\"n\\'t\" + 0.017*\"neutral\" + 0.014*\"page\"')\n",
      "(2, '0.042*\"wikipedia\" + 0.024*\"good\" + 0.024*\"admin\" + 0.023*\"work\" + 0.021*\"articles\" + 0.016*\"experience\" + 0.015*\"need\" + 0.013*\"admins\" + 0.013*\"policy\" + 0.011*\"contributions\"')\n",
      "(3, '0.065*\"yes\" + 0.060*\"vandal\" + 0.031*\"wikipedian\" + 0.025*\"nominate\" + 0.020*\"fighter\" + 0.017*\"sans\" + 0.016*\"p\" + 0.015*\"ms\" + 0.013*\"comic\" + 0.012*\"worthy\"')\n",
      "(4, '0.296*\"\\'\\'\" + 0.067*\"font\" + 0.047*\"support\" + 0.034*\"color=\" + 0.033*\"summaries\" + 0.032*\"style=\" + 0.021*\"span\" + 0.020*\"--\" + 0.016*\"—\" + 0.015*\"s\"')\n",
      "(5, '0.128*\"oppose\" + 0.045*\"--\" + 0.019*\"afd\" + 0.018*\"wp\" + 0.017*\"namespace\" + 0.014*\"reasons\" + 0.014*\"concerns\" + 0.013*\"deletion\" + 0.011*\"agree\" + 0.011*\"delete\"')\n",
      "(6, '0.041*\"\\'s\" + 0.025*\"n\\'t\" + 0.015*\"``\" + 0.014*\"\\'m\" + 0.014*\"oppose\" + 0.013*\"vote\" + 0.011*\"think\" + 0.010*\"admin\" + 0.010*\"rfa\" + 0.008*\"time\"')\n",
      "(7, '0.064*\"support\" + 0.043*\"small\" + 0.029*\"utc\" + 0.026*\"positive\" + 0.023*\"project\" + 0.021*\"sup\" + 0.018*\"benefit\" + 0.018*\"irc\" + 0.018*\"lucky\" + 0.014*\"talk\"')\n",
      "(8, '0.102*\"support\" + 0.053*\"n\\'t\" + 0.049*\"admin\" + 0.032*\"tools\" + 0.029*\"user\" + 0.027*\"--\" + 0.025*\"...\" + 0.025*\"reason\" + 0.019*\"\\'s\" + 0.018*\"use\"')\n"
     ]
    }
   ],
   "source": [
    "for topic in topics_9t:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = hlp.init_LDA_model(bow, d_r, num_topics=15)\n",
    "topics_9t = hlp.get_LDA_topics(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.219*\"good\" + 0.155*\"support\" + 0.058*\"work\" + 0.041*\"yes\" + 0.037*\"editor\" + 0.034*\"contributions\" + 0.029*\"nom\" + 0.019*\"happy\" + 0.018*\"record\" + 0.017*\"luck\"')\n",
      "(1, '0.034*\"agree\" + 0.029*\"strongly\" + 0.025*\"pov\" + 0.023*\"absolutely\" + 0.019*\"personal\" + 0.018*\"extreme\" + 0.017*\"changed\" + 0.017*\"comment\" + 0.014*\"possible\" + 0.014*\"nominee\"')\n",
      "(2, '0.078*\"edit\" + 0.063*\"summaries\" + 0.060*\"questions\" + 0.056*\"answers\" + 0.027*\"use\" + 0.023*\"sup\" + 0.022*\"history\" + 0.018*\"policy\" + 0.011*\"fair\" + 0.011*\"sense\"')\n",
      "(3, '0.056*\"vandal\" + 0.046*\"wikipedia\" + 0.043*\"new\" + 0.027*\"speedy\" + 0.025*\"deletion\" + 0.022*\"articles\" + 0.022*\"criteria\" + 0.018*\"fighter\" + 0.017*\"image\" + 0.016*\"deleted\"')\n",
      "(4, '0.113*\"—\" + 0.108*\"support\" + 0.079*\"\\'m\" + 0.071*\"sure\" + 0.040*\"\\'ll\" + 0.026*\"–\" + 0.017*\"lucky\" + 0.017*\"2005\" + 0.014*\"impressed\" + 0.012*\"soon\"')\n",
      "(5, '0.079*\"n\\'t\" + 0.067*\"\\'s\" + 0.032*\"support\" + 0.022*\"admin\" + 0.022*\"think\" + 0.016*\"vote\" + 0.015*\"like\" + 0.015*\"...\" + 0.014*\"\\'m\" + 0.013*\"rfa\"')\n",
      "(6, '0.039*\"oppose\" + 0.024*\"wikipedia\" + 0.018*\"admin\" + 0.016*\"n\\'t\" + 0.015*\"\\'s\" + 0.015*\"--\" + 0.014*\"good\" + 0.013*\"like\" + 0.013*\"edit\" + 0.013*\"experience\"')\n",
      "(7, '0.351*\"support\" + 0.184*\"--\" + 0.055*\"good\" + 0.033*\"great\" + 0.030*\"admin\" + 0.029*\"strong\" + 0.025*\"editor\" + 0.023*\"user\" + 0.020*\"like\" + 0.019*\"excellent\"')\n",
      "(8, '0.205*\"``\" + 0.163*\"\\'\\'\" + 0.023*\"vandals\" + 0.018*\"utc\" + 0.010*\"p\" + 0.008*\"voted\" + 0.008*\"...\" + 0.008*\"force\" + 0.007*\"guess\" + 0.007*\"days\"')\n",
      "(9, '0.090*\"articles\" + 0.068*\"article\" + 0.044*\"wp\" + 0.036*\"afd\" + 0.027*\"summary\" + 0.022*\"content\" + 0.020*\"irc\" + 0.018*\"work\" + 0.016*\"topics\" + 0.016*\"writing\"')\n",
      "(10, '0.217*\"oppose\" + 0.080*\"--\" + 0.051*\"neutral\" + 0.025*\"vote\" + 0.022*\"experience\" + 0.021*\"question\" + 0.021*\"concerns\" + 0.021*\"weak\" + 0.019*\"reasons\" + 0.018*\"answer\"')\n",
      "(11, '0.173*\"edits\" + 0.080*\"talk\" + 0.079*\"user\" + 0.062*\"page\" + 0.021*\"pages\" + 0.020*\"months\" + 0.017*\"edit\" + 0.017*\"article\" + 0.016*\"space\" + 0.015*\"wikipedia\"')\n",
      "(12, '0.094*\"support\" + 0.057*\"admin\" + 0.053*\"tools\" + 0.039*\"user\" + 0.029*\"use\" + 0.026*\"abuse\" + 0.017*\"trust\" + 0.016*\"editor\" + 0.016*\"wikipedia\" + 0.014*\"good\"')\n",
      "(13, '0.281*\"\\'\\'\" + 0.097*\"font\" + 0.054*\"support\" + 0.049*\"color=\" + 0.047*\"style=\" + 0.030*\"small\" + 0.030*\"span\" + 0.025*\"s\" + 0.024*\"--\" + 0.017*\"background\"')\n",
      "(14, '0.129*\"\\'ve\" + 0.093*\"support\" + 0.087*\"seen\" + 0.035*\"time\" + 0.030*\"nominator\" + 0.026*\"\\'s\" + 0.026*\"work\" + 0.025*\"cool\" + 0.023*\"lot\" + 0.014*\"mop\"')\n"
     ]
    }
   ],
   "source": [
    "for topic in topics_9t:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "topcis_3t_pipeline = get_LDA_topics_pipeline(comments_series, num_topics=3, ponctuation=True, stopwords=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.159*\"\\'\\'\" + 0.085*\"support\" + 0.045*\"--\" + 0.036*\"font\" + 0.025*\"``\" + 0.024*\"—\" + 0.018*\"color=\" + 0.018*\"style=\" + 0.016*\"looks\" + 0.016*\"small\"')\n",
      "(1, '0.033*\"oppose\" + 0.024*\"n\\'t\" + 0.020*\"\\'s\" + 0.014*\"edits\" + 0.009*\"user\" + 0.009*\"wikipedia\" + 0.009*\"time\" + 0.009*\"admin\" + 0.008*\"\\'m\" + 0.008*\"neutral\"')\n",
      "(2, '0.173*\"support\" + 0.066*\"--\" + 0.056*\"good\" + 0.027*\"admin\" + 0.023*\"user\" + 0.020*\"editor\" + 0.015*\"great\" + 0.013*\"work\" + 0.012*\"strong\" + 0.010*\"seen\"')\n"
     ]
    }
   ],
   "source": [
    "for topic in topcis_3t_pipeline:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "topcis_3t_pipeline = get_LDA_topics_pipeline(comments_series, num_topics=3, ponctuation=True, stopwords=True, fine_tune_stopwords=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.024*\"oppose\" + 0.016*\"edits\" + 0.012*\"wikipedia\" + 0.011*\"admin\" + 0.011*\"user\" + 0.009*\"edit\" + 0.009*\"time\" + 0.008*\"\\'m\" + 0.008*\"think\" + 0.007*\"like\"')\n",
      "(1, '0.063*\"oppose\" + 0.022*\"—\" + 0.020*\"questions\" + 0.019*\"small\" + 0.019*\"answers\" + 0.013*\"span\" + 0.013*\"neutral\" + 0.013*\"answer\" + 0.011*\"solid\" + 0.011*\"s\"')\n",
      "(2, '0.266*\"support\" + 0.057*\"good\" + 0.027*\"admin\" + 0.024*\"user\" + 0.020*\"great\" + 0.020*\"editor\" + 0.016*\"strong\" + 0.011*\"work\" + 0.011*\"contributor\" + 0.010*\"excellent\"')\n"
     ]
    }
   ],
   "source": [
    "for topic in topcis_3t_pipeline:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "topcis_3t_pipeline = get_LDA_topics_pipeline(comments_series, num_topics=3, ponctuation=True, stopwords=True, fine_tune_stopwords=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.035*\"oppose\" + 0.024*\"edits\" + 0.018*\"admin\" + 0.014*\"time\" + 0.014*\"wikipedia\" + 0.014*\"edit\" + 0.012*\"neutral\" + 0.012*\"good\" + 0.011*\"experience\" + 0.010*\"user\"')\n",
      "(1, '0.020*\"oppose\" + 0.012*\"page\" + 0.011*\"user\" + 0.010*\"vote\" + 0.008*\"talk\" + 0.006*\"...\" + 0.006*\"articles\" + 0.005*\"people\" + 0.005*\"wikipedia\" + 0.005*\"comments\"')\n",
      "(2, '0.269*\"support\" + 0.053*\"good\" + 0.025*\"admin\" + 0.021*\"user\" + 0.018*\"editor\" + 0.018*\"great\" + 0.016*\"strong\" + 0.013*\"—\" + 0.011*\"excellent\" + 0.010*\"work\"')\n"
     ]
    }
   ],
   "source": [
    "for topic in topcis_3t_pipeline:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_6t_pipeline = get_LDA_topics_pipeline(comments_series, num_topics=6, ponctuation=True, stopwords=True, fine_tune_stopwords=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.074*\"oppose\" + 0.057*\"edits\" + 0.042*\"edit\" + 0.028*\"talk\" + 0.028*\"user\" + 0.025*\"page\" + 0.012*\"months\" + 0.010*\"vandalism\" + 0.009*\"vandal\" + 0.009*\"count\"')\n",
      "(1, '0.270*\"support\" + 0.071*\"good\" + 0.032*\"user\" + 0.032*\"admin\" + 0.025*\"editor\" + 0.021*\"great\" + 0.014*\"contributor\" + 0.013*\"excellent\" + 0.012*\"seen\" + 0.012*\"like\"')\n",
      "(2, '0.036*\"yes\" + 0.018*\"color\" + 0.016*\"agree\" + 0.015*\"red\" + 0.013*\"green\" + 0.012*\"sam\" + 0.012*\"oh\" + 0.009*\"withdraw\" + 0.008*\"background\" + 0.007*\"e\"')\n",
      "(3, '0.032*\"wikipedia\" + 0.020*\"admin\" + 0.017*\"good\" + 0.016*\"time\" + 0.015*\"experience\" + 0.014*\"work\" + 0.014*\"oppose\" + 0.014*\"articles\" + 0.013*\"need\" + 0.012*\"like\"')\n",
      "(4, '0.028*\"oppose\" + 0.015*\"vote\" + 0.011*\"neutral\" + 0.011*\"admin\" + 0.010*\"think\" + 0.009*\"user\" + 0.009*\"rfa\" + 0.008*\"adminship\" + 0.007*\"like\" + 0.007*\"comments\"')\n",
      "(5, '0.171*\"support\" + 0.038*\"—\" + 0.030*\"...\" + 0.028*\"strong\" + 0.021*\"thought\" + 0.021*\"course\" + 0.016*\"answers\" + 0.015*\"questions\" + 0.015*\"nominator\" + 0.014*\"nom\"')\n"
     ]
    }
   ],
   "source": [
    "for topic in topics_6t_pipeline:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'tuple'>\n"
     ]
    }
   ],
   "source": [
    "print(type(topics_6t_pipeline))\n",
    "print(type(topics_6t_pipeline[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test.json\", \"w\") as file:\n",
    "    json.dump(topics_6t_pipeline, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test.json\", \"r\") as file:\n",
    "    topics_6t_read = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0,\n",
       "  '0.074*\"oppose\" + 0.057*\"edits\" + 0.042*\"edit\" + 0.028*\"talk\" + 0.028*\"user\" + 0.025*\"page\" + 0.012*\"months\" + 0.010*\"vandalism\" + 0.009*\"vandal\" + 0.009*\"count\"'],\n",
       " [1,\n",
       "  '0.270*\"support\" + 0.071*\"good\" + 0.032*\"user\" + 0.032*\"admin\" + 0.025*\"editor\" + 0.021*\"great\" + 0.014*\"contributor\" + 0.013*\"excellent\" + 0.012*\"seen\" + 0.012*\"like\"'],\n",
       " [2,\n",
       "  '0.036*\"yes\" + 0.018*\"color\" + 0.016*\"agree\" + 0.015*\"red\" + 0.013*\"green\" + 0.012*\"sam\" + 0.012*\"oh\" + 0.009*\"withdraw\" + 0.008*\"background\" + 0.007*\"e\"'],\n",
       " [3,\n",
       "  '0.032*\"wikipedia\" + 0.020*\"admin\" + 0.017*\"good\" + 0.016*\"time\" + 0.015*\"experience\" + 0.014*\"work\" + 0.014*\"oppose\" + 0.014*\"articles\" + 0.013*\"need\" + 0.012*\"like\"'],\n",
       " [4,\n",
       "  '0.028*\"oppose\" + 0.015*\"vote\" + 0.011*\"neutral\" + 0.011*\"admin\" + 0.010*\"think\" + 0.009*\"user\" + 0.009*\"rfa\" + 0.008*\"adminship\" + 0.007*\"like\" + 0.007*\"comments\"'],\n",
       " [5,\n",
       "  '0.171*\"support\" + 0.038*\"—\" + 0.030*\"...\" + 0.028*\"strong\" + 0.021*\"thought\" + 0.021*\"course\" + 0.016*\"answers\" + 0.015*\"questions\" + 0.015*\"nominator\" + 0.014*\"nom\"']]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_6t_read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Done with old version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,\n",
       " '0.144*\".\" + 0.135*\"support\" + 0.052*\"--\" + 0.033*\",\" + 0.029*\"good\" + 0.022*\"a\" + 0.019*\"-\" + 0.017*\"!\" + 0.015*\"and\" + 0.013*\"admin\"')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_3t = topics\n",
    "topics_3t[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Done with old version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_pipeline_6t = get_LDA_topics_pipeline(comments_series, num_topics=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,\n",
       " '0.065*\".\" + 0.043*\",\" + 0.040*\"and\" + 0.035*\"he\" + 0.033*\"a\" + 0.028*\"the\" + 0.026*\"i\" + 0.025*\"support\" + 0.023*\"to\" + 0.020*\"his\"')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_pipeline_6t[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Done with old version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_pipeline_9t = get_LDA_topics_pipeline(comments_series, num_topics=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,\n",
       " '0.079*\".\" + 0.049*\"of\" + 0.048*\",\" + 0.041*\"edits\" + 0.039*\"oppose\" + 0.035*\"and\" + 0.023*\"in\" + 0.017*\"a\" + 0.016*\"wikipedia\" + 0.016*\"experience\"')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_pipeline_9t[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/thetorf/anaconda3/envs/ada/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3526, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_72223/1189724256.py\", line 1, in <module>\n",
      "    topics_pipeline_3t_ws = get_LDA_topics_pipeline(comments_series, num_topics=3)\n",
      "  File \"/home/thetorf/Documents/epfl/MA1.2/ada/project-wikiRfA/ada-2023-project-abracadabra/P3_helpers.py\", line 374, in get_LDA_topics_pipeline\n",
      "    dictionary.filter_tokens(bad_ids=[dictionary.token2id[word] for word in STOPWORDS])\n",
      "  File \"/home/thetorf/Documents/epfl/MA1.2/ada/project-wikiRfA/ada-2023-project-abracadabra/P3_helpers.py\", line 374, in <listcomp>\n",
      "    dictionary.filter_tokens(bad_ids=[dictionary.token2id[word] for word in STOPWORDS])\n",
      "KeyError: 'ltd'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thetorf/anaconda3/envs/ada/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2120, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/home/thetorf/anaconda3/envs/ada/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1435, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/home/thetorf/anaconda3/envs/ada/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1326, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/home/thetorf/anaconda3/envs/ada/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1173, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/home/thetorf/anaconda3/envs/ada/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1088, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "  File \"/home/thetorf/anaconda3/envs/ada/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 970, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "  File \"/home/thetorf/anaconda3/envs/ada/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 792, in lines\n",
      "    return self._sd.lines\n",
      "  File \"/home/thetorf/anaconda3/envs/ada/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/home/thetorf/anaconda3/envs/ada/lib/python3.9/site-packages/stack_data/core.py\", line 698, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"/home/thetorf/anaconda3/envs/ada/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/home/thetorf/anaconda3/envs/ada/lib/python3.9/site-packages/stack_data/core.py\", line 649, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"/home/thetorf/anaconda3/envs/ada/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/home/thetorf/anaconda3/envs/ada/lib/python3.9/site-packages/stack_data/core.py\", line 628, in executing_piece\n",
      "    return only(\n",
      "  File \"/home/thetorf/anaconda3/envs/ada/lib/python3.9/site-packages/executing/executing.py\", line 164, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "topics_pipeline_3t_ws = get_LDA_topics_pipeline(comments_series, num_topics=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'topics_pipeline_3t_ws' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m topics_pipeline_3t_ws[\u001b[39m0\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'topics_pipeline_3t_ws' is not defined"
     ]
    }
   ],
   "source": [
    "topics_pipeline_3t_ws[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ada",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
